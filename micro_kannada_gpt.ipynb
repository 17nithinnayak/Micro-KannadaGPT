{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dhG-y5YQM28O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32        # Number of sequences processed in parallel\n",
        "block_size = 128       # Maximum context length (T)\n",
        "n_embd = 64           # Embedding dimension (C)\n",
        "n_head = 4            # Number of attention heads\n",
        "n_layer = 4           # Number of transformer blocks\n",
        "dropout = 0.1         # Dropout probability\n",
        "learning_rate = 3e-4  # AdamW learning rate\n",
        "max_iters = 2000      # Training iterations\n",
        "eval_interval = 100   # Print loss every N iterations\n",
        "eval_iters = 20       # Average loss over N batches for evaluation\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kfaO-HcNDT2",
        "outputId": "bfe225bf-56c0-4e2b-f351-272aa185ee69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c0cc0f284b0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kannada_text = \"\"\"\n",
        "ಹಸಿರು ಎಲೆಗಳ ನಡುವೆ ಹೂವು ಅರಳುತ್ತದೆ.\n",
        "ನದಿಯ ಹರಿವು ಎಂದಿಗೂ ನಿಲ್ಲುವುದಿಲ್ಲ.\n",
        "ಜ್ಞಾನವೇ ಶ್ರೇಷ್ಠ ಸಂಪತ್ತು.\n",
        "ಸತ್ಯವೇ ದೇವರು, ಧರ್ಮವೇ ಜೀವನ.\n",
        "ಮರದ ನೆರಳಿನಲ್ಲಿ ಪಕ್ಷಿಗಳು ಹಾಡುತ್ತವೆ.\n",
        "ಬೆಳಗಿನ ಸೂರ್ಯ ಎಲ್ಲರಿಗೂ ಸಮಾನ.\n",
        "ಪ್ರೀತಿ ಎಲ್ಲ ಬಾಧೆಗಳನ್ನು ದಾಟುತ್ತದೆ.\n",
        "ಕಲಿಕೆಯಲ್ಲಿ ವಯಸ್ಸಿಲ್ಲ.\n",
        "ಶಾಂತಿ ಮನಸ್ಸಿನಲ್ಲಿ ಹುಟ್ಟುತ್ತದೆ.\n",
        "ನಮ್ರತೆಯೇ ಶ್ರೇಷ್ಠ ಗುಣ.\n",
        "ಹೂವಿನ ಸುಗಂಧ ದೂರ ಹರಡುತ್ತದೆ.\n",
        "ಕಷ್ಟವೇ ಯಶಸ್ಸಿಗೆ ದಾರಿ.\n",
        "ಸಹಾನುಭೂತಿ ಮಾನವೀಯತೆಯ ಮೂಲ.\n",
        "ಕನಸುಗಳು ನಿಜವಾಗಲು ಶ್ರಮ ಬೇಕು.\n",
        "ಪ್ರಕೃತಿಯೇ ನಮ್ಮ ಗುರು.\n",
        "ಮಳೆಯ ಹನಿಗಳು ಭೂಮಿಯನ್ನು ತಣಿಸುತ್ತವೆ.\n",
        "ನಗು ಅತ್ಯುತ್ತಮ ಔಷಧ.\n",
        "ಸಮಯವೇ ಅಮೂಲ್ಯ ಧನ.\n",
        "ಸ್ನೇಹವೇ ಜೀವನದ ಆಧಾರ.\n",
        "ತಾಳ್ಮೆಯಿಂದ ಎಲ್ಲವೂ ಸಾಧ್ಯ.\n",
        "ಪುಸ್ತಕಗಳು ಜ್ಞಾನದ ಭಂಡಾರ.\n",
        "ಆಕಾಶವು ಮಿತಿಯಿಲ್ಲದ ವಿಸ್ತಾರ.\n",
        "ಸಂಗೀತ ಮನಸ್ಸಿಗೆ ಆಹಾರ.\n",
        "ಚಂದ್ರನ ಬೆಳಕು ರಾತ್ರಿಯನ್ನು ಬೆಳಗಿಸುತ್ತದೆ.\n",
        "ಕನ್ನಡವೇ ನಮ್ಮ ಹೆಮ್ಮೆಯ ಭಾಷೆ.\n",
        "ಎಲೆಗಳ ಸಪ್ಪಳ ಗಾಳಿಯ ಹಾಡು.\n",
        "ಸಾಗರದ ಅಲೆಗಳು ನಿರಂತರ ಚಲನೆ.\n",
        "ಬೆಳಗಾವಲು ಹೊಸ ಭರವಸೆಯ ಸಂಕೇತ.\n",
        "ತಾರೆಗಳು ಆಕಾಶದ ಆಭರಣ.\n",
        "ಮಾತು ಮನಸ್ಸಿನ ಕನ್ನಡಿ.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gpDmdlycNLAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, text):\n",
        "       chars = sorted(list(set(text)))\n",
        "       self.vocab_size = len(chars)\n",
        "       self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "       self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "       print(f\"Tokenizer initialized: {self.vocab_size} unique characters\")\n",
        "       print(f\"Sample chars: {chars[:10]}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert string to list of integers\"\"\"\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of integers back to string\"\"\"\n",
        "        return ''.join([self.idx_to_char[i] for i in indices])\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = CharTokenizer(kannada_text)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# Encode entire dataset\n",
        "data = torch.tensor(tokenizer.encode(kannada_text), dtype=torch.long)\n",
        "print(f\"Dataset size: {len(data)} characters\")\n",
        "\n",
        "# Train/val split (90/10)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_t3J1uyNONb",
        "outputId": "cf0b4c22-d180-45a2-daf7-ed1124f60127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer initialized: 45 unique characters\n",
            "Sample chars: ['\\n', ' ', ',', '.', 'ಂ', 'ಅ', 'ಆ', 'ಎ', 'ಔ', 'ಕ']\n",
            "Dataset size: 786 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_chars = list(tokenizer.char_to_idx.keys())\n",
        "print(\"Full Vocabulary:\")\n",
        "print(''.join(all_chars))\n",
        "\n",
        "# Print the character and its corresponding integer ID\n",
        "for char, idx in tokenizer.char_to_idx.items():\n",
        "    print(f\"Char: '{char}' | ID: {idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loxmUd4qPg9V",
        "outputId": "886cc034-26e0-49fa-ecd4-fb9fa01cdb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Vocabulary:\n",
            "\n",
            " ,.ಂಅಆಎಔಕಗಚಜಞಟಠಡಣತದಧನಪಬಭಮಯರಲಳವಶಷಸಹಾಿೀುೂೃೆೇೊ್\n",
            "Char: '\n",
            "' | ID: 0\n",
            "Char: ' ' | ID: 1\n",
            "Char: ',' | ID: 2\n",
            "Char: '.' | ID: 3\n",
            "Char: 'ಂ' | ID: 4\n",
            "Char: 'ಅ' | ID: 5\n",
            "Char: 'ಆ' | ID: 6\n",
            "Char: 'ಎ' | ID: 7\n",
            "Char: 'ಔ' | ID: 8\n",
            "Char: 'ಕ' | ID: 9\n",
            "Char: 'ಗ' | ID: 10\n",
            "Char: 'ಚ' | ID: 11\n",
            "Char: 'ಜ' | ID: 12\n",
            "Char: 'ಞ' | ID: 13\n",
            "Char: 'ಟ' | ID: 14\n",
            "Char: 'ಠ' | ID: 15\n",
            "Char: 'ಡ' | ID: 16\n",
            "Char: 'ಣ' | ID: 17\n",
            "Char: 'ತ' | ID: 18\n",
            "Char: 'ದ' | ID: 19\n",
            "Char: 'ಧ' | ID: 20\n",
            "Char: 'ನ' | ID: 21\n",
            "Char: 'ಪ' | ID: 22\n",
            "Char: 'ಬ' | ID: 23\n",
            "Char: 'ಭ' | ID: 24\n",
            "Char: 'ಮ' | ID: 25\n",
            "Char: 'ಯ' | ID: 26\n",
            "Char: 'ರ' | ID: 27\n",
            "Char: 'ಲ' | ID: 28\n",
            "Char: 'ಳ' | ID: 29\n",
            "Char: 'ವ' | ID: 30\n",
            "Char: 'ಶ' | ID: 31\n",
            "Char: 'ಷ' | ID: 32\n",
            "Char: 'ಸ' | ID: 33\n",
            "Char: 'ಹ' | ID: 34\n",
            "Char: 'ಾ' | ID: 35\n",
            "Char: 'ಿ' | ID: 36\n",
            "Char: 'ೀ' | ID: 37\n",
            "Char: 'ು' | ID: 38\n",
            "Char: 'ೂ' | ID: 39\n",
            "Char: 'ೃ' | ID: 40\n",
            "Char: 'ೆ' | ID: 41\n",
            "Char: 'ೇ' | ID: 42\n",
            "Char: 'ೊ' | ID: 43\n",
            "Char: '್' | ID: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    # Ensure the upper bound for randint is at least 1 to avoid 'from >= to' error\n",
        "    upper_bound = max(1, len(data_source) - block_size)\n",
        "    ix = torch.randint(upper_bound, (batch_size,))\n",
        "    x = torch.stack([data_source[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "yxWFODqCQ47F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        weights = q @ k.transpose(-2, -1)\n",
        "\n",
        "        weights = weights/(self.head_size ** 0.5)\n",
        "\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # Apply softmax to get attention probabilities\n",
        "        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # Weighted aggregation of values\n",
        "        # Shape: (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        out = weights @ v\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "rO7LfUyJ9e_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Droput(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "vng9K0qk_2y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "jq_3OtpNh2Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "u6ziymhI_6u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout) # Corrected typo: Droput -> Dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate outputs from all heads along the last dimension\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Fixed undefined 'out' and aggregation\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # This will now refer to the new MultiHeadAttention\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # Corrected typo: arrange -> arange\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Get actual sequence lengths for logits and targets\n",
        "            _, T_logits, C = logits.shape\n",
        "            _, T_targets = targets.shape\n",
        "\n",
        "            # Adjust logits or targets length to match for loss calculation\n",
        "            if T_logits > T_targets: # This is the case causing the error (idx longer than targets)\n",
        "                # Crop logits to match the length of targets\n",
        "                logits_to_use = logits[:, :T_targets, :].contiguous()\n",
        "                targets_to_use = targets.contiguous()\n",
        "            elif T_logits < T_targets: # Less expected, but handle for robustness (targets longer than logits)\n",
        "                # Crop targets to match the length of logits\n",
        "                logits_to_use = logits.contiguous()\n",
        "                targets_to_use = targets[:, :T_logits].contiguous()\n",
        "            else: # Lengths are already consistent\n",
        "                logits_to_use = logits.contiguous()\n",
        "                targets_to_use = targets.contiguous()\n",
        "\n",
        "            # Reshape for cross_entropy: (N, C) for logits and (N) for targets\n",
        "            loss = F.cross_entropy(logits_to_use.view(-1, C), targets_to_use.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Generate new tokens autoregressively.\n",
        "\n",
        "        Args:\n",
        "            idx: Starting context, shape (B, T)\n",
        "            max_new_tokens: Number of tokens to generate\n",
        "\n",
        "        Returns:\n",
        "            idx: Extended sequence, shape (B, T + max_new_tokens)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context to block_size\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "\n",
        "            # Focus on last time step\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "\n",
        "            # Append to sequence\n",
        "            idx = torch.cat([idx, idx_next], dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING LOOP\n",
        "# ============================================================================\n",
        "def train():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "\n",
        "    # Initialize model\n",
        "    model = GPTModel()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel initialized with {n_params:,} parameters\")\n",
        "    print(f\"Training on {device}\\n\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\\n\")\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "        # Evaluate loss periodically\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(model)\n",
        "            print(f\"Step {iter:4d} | Train loss: {losses['train']:.4f} | Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "        # Sample batch\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # Forward pass\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training complete!\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TEXT GENERATION\n",
        "# ============================================================================\n",
        "def generate_text(model, prompt=\"\", max_tokens=200):\n",
        "    \"\"\"Generate text from the model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode prompt or start with newline\n",
        "    if prompt:\n",
        "        context = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n",
        "    else:\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate\n",
        "    generated_ids = model.generate(context, max_new_tokens=max_tokens)[0].tolist()\n",
        "    generated_text = tokenizer.decode(generated_ids)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"MICRO-KANNADAGPT: Transformer from Scratch\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nHyperparameters:\")\n",
        "    print(f\"  - Embedding dim: {n_embd}\")\n",
        "    print(f\"  - Attention heads: {n_head}\")\n",
        "    print(f\"  - Transformer layers: {n_layer}\")\n",
        "    print(f\"  - Context length: {block_size}\")\n",
        "    print(f\"  - Batch size: {batch_size}\")\n",
        "    print(f\"  - Learning rate: {learning_rate}\")\n",
        "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
        "    print(f\"  - Device: {device}\")\n",
        "\n",
        "    # Train the model\n",
        "    model = train()\n",
        "\n",
        "    # Generate sample text\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAMPLE GENERATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nGeneration 1 (from scratch):\")\n",
        "    print(\"-\" * 60)\n",
        "    output1 = generate_text(model, prompt=\"\", max_tokens=150)\n",
        "    print(output1)\n",
        "\n",
        "    print(\"\\n\\nGeneration 2 (with prompt):\")\n",
        "    print(\"-\" * 60)\n",
        "    output2 = generate_text(model, prompt=\"ಜ್ಞಾನ\", max_tokens=100)\n",
        "    print(output2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\u2713 Project complete! Model successfully trained and generated text.\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "uhu0EpW2B5A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2961f1e-4302-4ec2-d0c7-73a56020596b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MICRO-KANNADAGPT: Transformer from Scratch\n",
            "============================================================\n",
            "\n",
            "Hyperparameters:\n",
            "  - Embedding dim: 64\n",
            "  - Attention heads: 4\n",
            "  - Transformer layers: 4\n",
            "  - Context length: 128\n",
            "  - Batch size: 32\n",
            "  - Learning rate: 0.0003\n",
            "  - Vocabulary size: 45\n",
            "  - Device: cuda\n",
            "\n",
            "Model initialized with 213,293 parameters\n",
            "Training on cuda\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Step    0 | Train loss: 3.9803 | Val loss: 4.0110\n",
            "Step  100 | Train loss: 2.5936 | Val loss: 3.1121\n",
            "Step  200 | Train loss: 2.1880 | Val loss: 3.1283\n",
            "Step  300 | Train loss: 1.8508 | Val loss: 3.3132\n",
            "Step  400 | Train loss: 1.4498 | Val loss: 3.6921\n",
            "Step  500 | Train loss: 1.0559 | Val loss: 4.1341\n",
            "Step  600 | Train loss: 0.6863 | Val loss: 4.5825\n",
            "Step  700 | Train loss: 0.4299 | Val loss: 4.9145\n",
            "Step  800 | Train loss: 0.2782 | Val loss: 5.1897\n",
            "Step  900 | Train loss: 0.1947 | Val loss: 5.5532\n",
            "Step 1000 | Train loss: 0.1411 | Val loss: 5.7161\n",
            "Step 1100 | Train loss: 0.1043 | Val loss: 5.9256\n",
            "Step 1200 | Train loss: 0.0858 | Val loss: 5.9579\n",
            "Step 1300 | Train loss: 0.0718 | Val loss: 6.0989\n",
            "Step 1400 | Train loss: 0.0634 | Val loss: 6.1017\n",
            "Step 1500 | Train loss: 0.0575 | Val loss: 6.2293\n",
            "Step 1600 | Train loss: 0.0510 | Val loss: 6.3260\n",
            "Step 1700 | Train loss: 0.0495 | Val loss: 6.3942\n",
            "Step 1800 | Train loss: 0.0453 | Val loss: 6.4851\n",
            "Step 1900 | Train loss: 0.0421 | Val loss: 6.5928\n",
            "Step 1999 | Train loss: 0.0405 | Val loss: 6.6117\n",
            "\n",
            "============================================================\n",
            "Training complete!\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "SAMPLE GENERATION\n",
            "============================================================\n",
            "\n",
            "Generation 1 (from scratch):\n",
            "------------------------------------------------------------\n",
            "\n",
            "ಹರಡುತ್ತದೆ.\n",
            "ಕಷ್ಟವೇ ಯಶಸ್ಸಿಗೆ ದಾರಿ.\n",
            "ಸಹಾನುಭೂತಿ ಮಾನವೀಯತೆಯ ಮೂಲ.\n",
            "ಕನಸುಗಳು ನಿಜವಾಗಲು ಶ್ರಮ ಬೇಕು.\n",
            "ಪ್ರಕೃತಿಯೇ ನಮ್ಮ ಗುರು.\n",
            "ಮಳೆಯ ಹನಿಗಳು ಭೂಮಿಯನ್ನು ತಣಿಸುತ್ತವೆ.\n",
            "ನಗು ಅತ್ಯು\n",
            "\n",
            "\n",
            "Generation 2 (with prompt):\n",
            "------------------------------------------------------------\n",
            "ಜ್ಞಾನವೇ ಶ್ರೇಷ್ಠ ಗು.\n",
            "ಸಂಧ ದೂರ ಹರಡುತ್ತದೆ.\n",
            "ಕಷ್ಟವೇ ಯಶಸ್ಸಿಗೆ ದಾರಿ.\n",
            "ಸಹಾನುಭೂತಿ ಮಾನವೀಯತೆಯ ಮೂಲ.\n",
            "ಕನಸುಗಳು ನಿಜವಾಗಲು ಶ್\n",
            "\n",
            "============================================================\n",
            "✓ Project complete! Model successfully trained and generated text.\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}